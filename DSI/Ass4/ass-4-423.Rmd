---
title: "Assignment 4 DATA423-20S1"
author: "Zhen Huang"
date: "5/22/2020"
output:
  html_notebook: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1-Caret

### Overview

#### Caret

The `caret` package which described, by its first author/maintainer Max Kuhn, as a unified interface of predictive models. To be specific, it provides a set of tools (functions and classes) that attempt to streamline the process for creating classification and regression models.^[The caret Package  http://topepo.github.io/caret/index.html]
The package contains tools such as data splitting, pre-processing and resampling, feature selection, model tuning, performance evaluation, models comparison, visualization for the models and the other functionality.

#### Python's scikit-learn

Scikit-learn is an open-source machine learning library in Python that supports supervised and unsupervised learning. Similar to `caret`, it also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.^[scikit-learn.org https://scikit-learn.org/stable/getting_started.html]

#### R's mlr3 package

One of the design principles of `mlr3` is focusing on computation. Together, the `mlr3` package and its ecosystem provide a generic framework for machine learning tasks for the R language. Similar to `caret` wrapping function `caret::train`, `mlr3` also provide a unified interface to many popular machine learing algorithms in R, which are objects of class `mlr3::learner`.
^[mlr3 Manual https://mlr3book.mlr-org.com/introduction.html]

### Compare and Contrast the Frameworks

#### Tasks coverage

The general data science tasks were taken into account in a comparison of functions coverage, which is shown in the below table^[*(asterisk) means functions or classes integrated in the package]. 

|Tasks Coverage|`caret`|`scikit-learn`|`mlr3`|
|-|-|-|-|
|Visualisation|*|`matplot`|`viz`|
|Pre-processing|* or work with `recipe`|* work with `numpy`|`pipelines`|
|Data spliting|*|*|`pipelines`|
|Standardised training/predicting|wrapping |*|wrapping|
|Feature engineering/selection|*|*|`filter`,`fswrap`|
|Method categorisation|*|||
|Resamplling|*|*|*|
|Hyperparameter tuning|*|*|`tuning`,`hyperband`,`mbo`,`paradox`|
|Model evaluation|*|*|`measures`|
|Model selection|*|*||
|Parallel Processing|*|*|*|

Except for method categorisation and model selection, all three packages can handle most of the tasks in predicting models. There is much difference in details worthing to discuss, such as the way the packages achieving the tasks and specific functions or classes within these packages. 

For the **visualisation** task, `caret` offers its own functions which suitable to the specific model,  `scikit-learn` leaves this part of the job to `matplot` and `mlr3` offers through its extension package `viz`. This kind of extension function makes up the `mlr3` ecosystem, which is designed by following the principles in the `mlr3`.

It is critical for predicting a model by implementing an efficient and reasonable **pre-processing** step dealing with missing data, variable encoding and various feature transformations. Although three packages have their own tools, it is usually a good practice working together with other packages, such as `recipe` and `caret`, `numpy` and `scikit-learn`. There is a similar idea of `pipelines` in `mlr3` and `scikit-learn`, furthermore, `pipelines` in `mlr3` offers a more complicated structure by networking the pre-processing steps together in `Graph`, which is not only a visualisation for the inputs and outputs but making it possible to connect the steps of training/predicting/hyperparameter tuning with pre-processing together.

Both `caret` and `mlr3` wrap the methods of other packages in R, while `scikit-learn` implements a method in itself. So, depending packages need to be loaded when training/predicting models in `caret` and `mlr3` explicitly or implicitly.

Both `caret` and `scikit-learn` offer a range of options in feature selection/extraction/transformation/composition whatever they called in the packages. At the same time `mlr3` has less choice at the present and new extension `mlr3fselect` in an experiment. 

Similar situations show in the tasks of resampling, hyperparameter tuning, model evaluation and model selection when comparing three packages in these tasks.

There is a very well **method categorisation** in `caret` in terms of both the quality of documents and helping tools.

**Parallel Processing** is a critical ability when facing a computational problem or a big dataset when the strategies to scale computationally, such as decreasing the data size, is not enough. There is much difference in the three packages. Basically, `caret` and `scikit-learn` utilize the other backend packages in R and Python, respectively, to leverage the parallel processing framework ^[https://topepo.github.io/caret/parallel-processing.html] ^[https://scikit-learn.org/stable/modules/computing.html#computational-performance]. These type of parallelism is based on using multiple CPU cores in a local machine. `mlr3` uses the `future` backends for parallelisation ^[https://mlr3book.mlr-org.com/parallelization.html#parallelization] which is different with `caret`. `future` enables `mlr3` has the ability to run on one or more local or remote machines. `spark-sklearn`  is another attemp to extending `scikit-learn` in Python world focuses on problems that have a small amount of data, and that can be run in parallel ^[https://pypi.org/project/spark-sklearn/] in Spark's distributed computational framework. 

#### Models

A simple comparison with the number of methods in three packages is shown below.
^[http://topepo.github.io/caret/available-models.html] ^[https://scikit-learn.org/stable/user_guide.html] ^[https://mlr3book.mlr-org.com/list-learners.html] 

||`caret`|`scikit-learn`|`mlr3`|
|-|-|-|-|
|Number of methods|238|140|51|

The very newcomer is `mlr3`, launched from 2019, which supports fewer models than the other two packages. Under these simple numbers, `caret` focuses on the classification and regression methods, just as its acronym. Note that only `scikit-learn` supports cluster methods at present, while `mlr3` is going to achieve this through its extension package `mlr3cluster` in progress. So if we are facing an unsupervised clustering problem or anomoly detection, `scikit-learn` is the most appropriate choice with its nearest neighbour and novelty and outlier detection methods.

#### Strengths and weaknesses

From the comparison above we have a basic knowledge of three packages. Here below are the summary and some additional discussion in their strengths/weaknesses and limitations.

For `caret`, the wide range of choices in classification and regression methods is the most outstanding strength compared with the other two packages. The ability of processing data and predicting model is limited by physical components in the local machine, which may be a bottleneck of the predicting in facing big data. In addition, there is less flexible in building a customed deep learning neuron network, which limits the ability of `care` in handling with image/audio/video segmentation/classification problem.

For `scikit-learn`, the diversity of its methods option is one of the advantages, such as clustering and outliers detection method. Scaling and parallelism limitation from the local machine also exist in `scikit-learn`. However, with the background of Python, `scikit-learn` can extend its application to Spark and empower the ability of distributed processing. As the changing interest in the python open source community, more contributes now toward PyTorch, Dash, which face the modern data science problems. This trend leads `scikit-learn` will focus on improving as an external components, which can be integrated into these frameworks. ^[https://scikit-learn.org/dev/roadmap.html]

The first release of `mlr` is in 2013, while the rewritten `mlr3` has the ambition to build an ecosystem with its extendible structure including modern features in R including R6 objects, `future` backend and learners suitable to build neuron networks, such as `mlr3keras` in experimental.


---

## Question 2 - Report assessment (Marks: 1/3)

### The data

The dataset of the project had 80773 observations and 21 raw variables before the pre-processing. There are five data types of these variables. Two new numeric variables were added by using feature engineering, and one of them was assigned to be the outcome variable later.

### Procedure followed

#### Missing and imputation

1. Providing the proportion of missing data, however, further investigation should be taken to determine whether there is a potential pattern of missing data in specific variables, observations , or just missing at random. 

2. The imputation was ahead of the training/test split and on all the data. This imputation would lead to "data leakage", mainly, when missing data is not at random. 

3. The method of imputation was not presented in the report. The possibility of omitting the missing data was not discussed in the report, whether omitting would be better than imputation?

4. Whether the observations were removed automatically or manually?

#### Outliers

5. The method and criteria detecting outliers were not discussed in the report. Whether the outliers had a pattern in single variable or multivariable?

6. Removing the potential outlier without given any explanations. Outliers deletion can only occur in the case that outliers will not occur in the future unseen data.

#### Feature engineering

7. An interaction terms "BodyMassIndex" was added without showing the explanation and examination of the relationship between "Weight" and "Height". Although this is intuitive, some exception might exist in the dataset.

8. "Net_productivity" is a combination of three variables whose relationship to each other should also be examed. Although related variables removed from predictors later, the "Sales", "Returned_sales" might relate to "Sales_value" and "Returned_sales_value". High "Sales" or low "Returned_sales" would be the effect of "Net_productivity". Thus, the potential data leakage might exist in predicting the model. 

#### Role assignment

9. More investigation could be done when assigning the roles of variables. As the description of variables, "Branch" is a globally unique branch name, thus, "Branch"+"Employee ID" could be a unique observation identifier. Then "Country" might be kept as a predictor in this study because there was no other variable related to certain Nationalities that was part of the confirmation in this study.

(There may have a typo in the role assignment at "Company" which does not exist in the dataset.)

#### Feature Selection

10. The "Weight" and "Height" were kept in the predictors, while they consisted the interaction variable "BodyMassIndex". There was no discussion for the selection in the study. Whether "BodyMassIndex" is essential, and whether the interpretability changed, if "Weight" and "Height" were removed.

11. "Shift_length" was kept in the predictor, however, it was one of the variable consisted of the outcome variable. The outcome consisted of one predictor would lead to data leakage as mentioned above.

12. There were 17 predictors left. The number of predictors could lead to the curse of dimension to one of the candidate method, KNN. If there is a relationship between the other numeric predictors, PCA can be used to reduce the number of variables. If there is a relationship between outcome and predictor, PLS can be used either.

#### Standardising

13. Standardising before the training/test split potentially leaks information from the test to train dataset.

#### Converting/Encoding

14. `model.matrix` converted nominal variables into binary dummy-variables for the dataset. One more intercept column added also was added into the dataset which was suitable for a linear regression model, however, neither intercept column nor dummy-variables are necessary for a random forest model. There was no explanation whether this transformation was used in training the random forest model.

#### Resampling

15. Consider there were 80773 rows in the dataset and 17 variables, a train-validation-test split could be a better alternative than simple random split into training/test.

15. The potential risk of near-zero variance was not examed in the study, so a one-time train/test split was running the risk of NZV. 

16. A supervised stratified split would be a better alternative, if a categorical variable proved to be an appropriate group data role.

#### Modelling candidates

17. There is no explanation about why choosing these as candidate methods. For example, a better practice can be reporting a list of available methods for the problem and shows how to do the filtering.

18. Hyperparameters chosen for the methods; however, there was one set of hyperparameter in GLMnet and Neural network methods, neither evidence nor the process were shown in the report of how these hyperparameters were chosen. For KNN and Random Forest, two different sets were shown without evidence in proving which one was better.

19. Neither the results nor the process of training was reported, if tuning grid was used, was there a problem of the hyperparameter at the edge of the grid?

20. Neural network is a generic name of a batch of methods/algorithms; the specific method should be reported.

### Conclusion

21. As the resampling method was a one-time train/test split in the study, using test RMSE to choose the best model exclusively was not the best practice. Using one of cross-validation resampling method would be an improvement.

22. The test results of the other candidate methods were not reported. If the test RMSE of one method was close to the best model, and it has an advantage of interpretability over neuron network, the model was also worth to report.

23. The residuals had not to be examed whether there was a pattern which suggests the model had not explained some other information in the dataset.

24. The statistics of the outcome can be compared with the RMSE to show whether the model predicted sensible values.

25. No guarantee of the model always performs the best application. For example, there was no evidence that the model would be tolerant of the novelty of categorical data or missing data in the future unseen data.

26. The economic statuses vary from different country; thus, it was not a fair comparison in the whole dataset without stratified or grouped employees. Gender and religion also related to a certain group of employees which should be considered in the study, such as stratified resampling or fitting models for each group.

---

## Question 3 - Quality Control (Marks: 1/3)
The file monitor.csv contains comma separated data. The columns are

Timestamp              - the timestamp of a model prediction being run
ProcessMemory      - the allocated memory (MB) of the relevant server process
Prediction                - the value predicted by the model
PredictionTimeMS   - the duration of the prediction task in milliseconds
Using the supplied CSV data, generate control charts and answer the following questions:

a) Is the memory usage of the server in control?

b) Is the prediction time of the model in control?

c) Is the stream of predictions in control?

The relevant control charts would be “xbar” and “s”. You should aggregate the data per day. Assume the first 20 days of data can be used to establish the control limits for the remainder of the data.

Remember to report upon runs.signal and sigma.signal from each chart summary.

Have a read of https://cran.r-project.org/web/packages/qicharts/vignettes/controlcharts.html
Present your charts and results as an R-Notebook document.  In RStudio use the menu choices: File / New File / R Notebook.



